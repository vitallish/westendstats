{
  "hash": "4a5c0ac518dc208fe0eee6045fe85f2c",
  "result": {
    "markdown": "---\ntitle: \"Conditional MLE Variance\"\nauthor: \"Vitaly Druker\"\ndate: \"2016-11-01\"\ncategories: [R, simulation, MLE, casi]\n---\n\n### Introduction  \nThe most challenging (and useful) work in statistics comes not from trying to estimate a value, but from estimating the _accuracy_ of that value. The most famous example of this is that for a normally distributed variable \n$$\\hat{\\mu} \\sim  \\mathcal{N}\\left( \\mu, \\sigma^2/n\\right)$$ \nwhere $\\hat{\\mu}$ is an estimate of $\\mu$ and $n$ is the number of samples taken.\nThe formula above is actually a generalization of \n$$\\hat{\\theta} \\sim  \\mathcal{N}\\left( \\theta, 1/(n\\mathcal{I}_\\hat{\\theta})\\right)$$ \nwhere $\\mathcal{I}_\\hat{\\theta}$ is the Fisher Information for the Normal Distribution. $\\hat{\\theta}$ can represent more than one parameter to be estimated. In our specific example, $\\mathcal{I}_\\hat{\\theta} = 1/\\sigma^2$. One important note about $\\mathcal{I}_\\hat{\\theta}$ is that it does not explicitly depend on the data being analyzed. \nAs a result of this issue, Fisher proposed a slightly different statistic, called the _Observed Fisher Information_ ($I(x)$) which depends on the data observed.\n$$ I\\left(x\\right) = -\\ddot{l}_x(\\hat{\\theta}) = -\\frac{\\partial^2}{\\partial \\theta^2}l_x\\left(\\theta\\right)\\vert_{\\hat\\theta}$$ \nwhere $l_x\\left(\\theta \\right)$ is the log-likelihood function for the distribution that the data is drawn from and is evaluated at the maximum likelihood estimator $\\hat\\theta$.\n$I(x)$ and $\\mathcal{I}_\\hat{\\theta}$ are related by the fact the \n$$E[I(x)] = n\\mathcal{I}$$ \nThe section above was adapted from Chapter 4.3 in the excellent [Computer Age Statistical Inference][efron_hastie] which explains maximum likelihood (ML) in the most accessible way I've seen. What follows next is a recreation of Figure 4.2 (pg 48) which illustrates the difference in the Conditional (Observed) Fisher Information and the Expected Fisher Information\n\n### Simulation Comparing Observed and Expected Fisher Information  \n\n#### Estimating $\\hat{\\theta}$ using MLE\n\n\n\n\n\nThe example that follows is for the Cauchy distribution which follows the distribution\n$$f_\\theta(x) = \\frac{1}{\\pi} \\frac{1}{1+(x-\\theta)^2}$$ \nThis is a form of the Cauchy distribution where the scale parameter equals 1. The likelihood equation is\n$$-log(f_\\theta(x)) = -(log(\\frac{1}{\\pi}) + log(1) -log(1 + (x - \\theta)^2))$$\nWe can then drop all of the constant terms and our final equation for calculating the maximum likelihood is\n$$ l_x(\\theta) = \\sum_{i=1}^{n}{log(1 + (x_i - \\theta)^2)}$$ \nwhich is defined in R here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlcauchy <- function(theta, x){\n  sum(log(1 + (x - theta)^2))\n}\n```\n:::\n\nWe can then use the `nlm` function to find the MLE for $\\theta$. `location` in the code below is our true $\\theta$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1120)\ncauchy_samp <- rcauchy(n = 20, location = 0, scale = 1)\nmle_estimate <- nlm(lcauchy, p = median(cauchy_samp), \n                    x = cauchy_samp)\n\nround(mle_estimate$estimate,2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.13\n```\n:::\n:::\n\n\n#### Calculating Expected Fisher Information\nThe Expected Fisher Information can be shown to be\n$$\\mathcal{I}_\\hat{\\theta} = \\frac{1}{2}$$ \nmeaning the the variance of the estimate of $\\hat{\\theta}$ is bound by variance of\n$$\\frac{1}{n\\mathcal{I}} = \\frac{2}{n}$$. \nThe full proof can be found in [solution (i) here][expected_fisher]. Note that this does not depend on the data observed (only the number of point $n$)  \n\n#### Calculating the Observed Fisher Information  \nThe observed Fisher Information may be calculated based off its definitions above. For the Cauchy distribution\n$$ I(x) = -2\\sum_{i=1}^{n}{\\frac{(x_i-\\hat\\theta)^2-1}{1+(x_i-\\hat\\theta)^2)^2}}$$ \nwhich is defined below in R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobsInformCauchy <- function(theta, x){\n  sum((-2 * ((x - theta) ^ 2 - 1)) / (1 + (x - theta) ^ 2) ^ 2)\n}\n```\n:::\n\n\nThis suggests that $\\hat{\\theta}$ is bound by variance of \n$$\\frac{1}{I(x)}$$\nAs before please note that this  _does_ depend on the data collected.\n\n#### Simulation comparing the three variances\nThe simulation below will create 10,000 samples of 20 from a Cauchy distribution. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarCauchy <- function(n) {\n  # Create a sample size n from cauchy distribution\n  cauchy_samp <- rcauchy(n)\n  \n  # Compute the MLE of theta\n  mle_estimate <-\n    nlm(f = lcauchy, \n        p = median(cauchy_samp),\n        x = cauchy_samp)\n  theta_hat <-  mle_estimate$estimate\n  \n  # Calculate the Observed Fisher Information at the MLE of theta\n  fish <- obsInformCauchy(theta_hat, cauchy_samp)\n  \n  # Return the MLE and Observed Fisher Information\n  c(t_h = theta_hat,\n    obs_fish = fish)\n}\n\nlibrary(dplyr)\nsims <- sapply(1:10000, function(x) varCauchy(20)) %>% \n  t %>% \n  as.data.frame\n\npander::pander(head(sims), style = 'rmarkdown')\n```\n\n::: {.cell-output-display}\n|   t_h    | obs_fish |\n|:--------:|:--------:|\n| -0.07252 |  8.568   |\n| 0.07057  |  12.93   |\n|  -0.149  |  6.096   |\n| -0.08777 |  9.966   |\n|  0.1799  |  7.437   |\n|  0.5064  |   11.2   |\n:::\n:::\n\n\n\nWe will then calculate the variance based off the observed fisher (column `bound` below). Finally, we will group the the variance into deciles `group` and calculate the actual variance from the MLE (`var_mle`). \n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- sims %>% \n  mutate(bound = 1/obs_fish, \n         group = cut(bound, \n                     breaks = quantile(bound, \n                                       probs = seq(0,1,.1)),\n                     ordered_result = T)) %>% \n  group_by(group) %>% \n  summarise(avg_bound = mean(bound),\n            var_mle = var(t_h))\n\npander::pander(df, style = 'rmarkdown')\n```\n\n::: {.cell-output-display}\n|      group      | avg_bound | var_mle |\n|:---------------:|:---------:|:-------:|\n| (0.0407,0.0652] |   0.059   | 0.07185 |\n| (0.0652,0.0728] |  0.06929  | 0.07661 |\n| (0.0728,0.0798] |  0.0764   | 0.0852  |\n| (0.0798,0.0868] |  0.0832   | 0.09435 |\n| (0.0868,0.094]  |  0.09029  | 0.1075  |\n|  (0.094,0.102]  |  0.09799  | 0.1125  |\n|  (0.102,0.114]  |  0.1077   | 0.1199  |\n|  (0.114,0.129]  |  0.1213   | 0.1323  |\n|  (0.129,0.157]  |  0.1411   | 0.1537  |\n|  (0.157,1.15]   |  0.2163   |  0.244  |\n|       NA        |  0.0407   |   NA    |\n:::\n:::\n\n\nWe will then plot the average estimated variance `avg_bound` against the actual variance `var_mle` along with the variance given by the expected fisher information.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\np <- df %>% \n  ggplot(aes(x = avg_bound, y = var_mle, color = \"Simulated Values\")) + \n  geom_point() + \n  geom_hline(aes(yintercept = 0.1, \n             color = \"Expected Value\"),\n             linetype = 2,\n             show.legend = T)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](conditional-mle-variance_files/figure-html/unnamed-chunk-6-1.png){width=100%}\n:::\n:::\n\n\nThe graph above clearly shows that the Observed Information Bound is much better at assessing MLE variability. It's important to note that as $n$ gets larger, this difference becomes less and less important. Nevertheless, this is a great argument for taking the actual data into account when computing things like confidence intervals. \n\n### Further Reading  \n1. [Computer Age Statistical Inference][efron_hastie] provided the initial reason for exploring this on page 48. Additionally, an older manuscript (1978!) from the same authors is available [for free online](https://www.stat.tamu.edu/~suhasini/teaching613/expected_observed_information78.pdf). See the graph on page 460 of the pdf.  \n2. [Geyer, 2003][geyer_mle] provided some much needed details on how to calculate the fisher information in R. \n3. A step by step proof on how to calculate the Expected Fisher Information (warning: it's awful) can be [found here in the solution to (i)][expected_fisher]\n\n[efron_hastie]: https://web.stanford.edu/~hastie/CASI/\n[geyer_mle]: http://www.stat.umn.edu/geyer/5931/mle/mle.pdf\n[expected_fisher]: http://wwwf.imperial.ac.uk/~das01/MyWeb/M3S3/Handouts/WorkedExample-Cauchy.pdf",
    "supporting": [
      "conditional-mle-variance_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}