[
  {
    "objectID": "wip/bayesian-power-trial/index.html",
    "href": "wip/bayesian-power-trial/index.html",
    "title": "Bayesian Power Trial",
    "section": "",
    "text": "Try to recreate power analysis in Böhm et al. (2020) - renal denervation trial that uses this method\n\nSpecifically the ON-MED group\nNote that in the table below I transformed the standard errors found in table 3 of the publication for the pilot study.\n\nparameter_assumptions &lt;- tibble::tribble(\n    ~trial,    ~arm,        ~basline_adjusted_mean, ~baseline_adjusted_sd, ~n,\n    \"pilot\",   \"treatment\", -8.8,                   1.8*sqrt(36),          36,\n    \"pilot\",   \"control\",   -1.8,                   1.8*sqrt(36),          36,\n    \"pivotal\", \"treatment\", -6.8,                   12,                    NA,\n    \"pivotal\", \"control\",   -1.8,                   12,                    NA\n\n)\n\n\nWeibull discount functyion parameters that were used: Shape \\(k = 3\\), scale \\(\\lambda =0.25\\)\nInterim analyses will happen at 175 and 220 subjects\nTreatment effect defined by:\n\\(\\mu = \\mu_t - \\mu_c\\)\nTrial success criteria:\n\\[\nP(\\mu &lt;0 ) \\gt .975\n\\]\nTrial futility is made by imputation of remaining subjects and if\n\\[\nP(\\mu &lt;0 ) \\lt .05\n\\]\n\n\n\nOverall trial pwer to detect treatment difference of -5 was 96%\nType I error 3%\nPower at first and second interim looks was 89% and 94%"
  },
  {
    "objectID": "wip/bayesian-power-trial/index.html#overview-of-spyral-htn-on",
    "href": "wip/bayesian-power-trial/index.html#overview-of-spyral-htn-on",
    "title": "Bayesian Power Trial",
    "section": "",
    "text": "Try to recreate power analysis in Böhm et al. (2020) - renal denervation trial that uses this method\n\nSpecifically the ON-MED group\nNote that in the table below I transformed the standard errors found in table 3 of the publication for the pilot study.\n\nparameter_assumptions &lt;- tibble::tribble(\n    ~trial,    ~arm,        ~basline_adjusted_mean, ~baseline_adjusted_sd, ~n,\n    \"pilot\",   \"treatment\", -8.8,                   1.8*sqrt(36),          36,\n    \"pilot\",   \"control\",   -1.8,                   1.8*sqrt(36),          36,\n    \"pivotal\", \"treatment\", -6.8,                   12,                    NA,\n    \"pivotal\", \"control\",   -1.8,                   12,                    NA\n\n)\n\n\nWeibull discount functyion parameters that were used: Shape \\(k = 3\\), scale \\(\\lambda =0.25\\)\nInterim analyses will happen at 175 and 220 subjects\nTreatment effect defined by:\n\\(\\mu = \\mu_t - \\mu_c\\)\nTrial success criteria:\n\\[\nP(\\mu &lt;0 ) \\gt .975\n\\]\nTrial futility is made by imputation of remaining subjects and if\n\\[\nP(\\mu &lt;0 ) \\lt .05\n\\]\n\n\n\nOverall trial pwer to detect treatment difference of -5 was 96%\nType I error 3%\nPower at first and second interim looks was 89% and 94%"
  },
  {
    "objectID": "wip/bayesian-power-trial/index.html#other-publications",
    "href": "wip/bayesian-power-trial/index.html#other-publications",
    "title": "Bayesian Power Trial",
    "section": "Other publications",
    "text": "Other publications\n\nlibrary(bayesDP) # package that was used in the clinical trial\n\nLoading required package: ggplot2\n\n\nLoading required package: survival\n\n\nHaddad et al. (2017) for perspective from device community\nspecifics about dynamic borrowing Viele et al. (2014)\ntest"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "West End Statistics was founded by Vitaly Druker in 2019 and has helped clients with clinical study design, study analysis and research. West End Statistics (WES) specializes in complex designs that require simulated data. WES focuses on communicating effectively with clients and assissting them with making design decisions that give them the best chance of meeting their goals."
  },
  {
    "objectID": "posts/cutting-with-trees/index.html",
    "href": "posts/cutting-with-trees/index.html",
    "title": "Cutting with Trees",
    "section": "",
    "text": "Motivation\nAs I have mentioned before, the most useful information statistics can provide us is a measure of uncertainty in a result. Arguably the most common way that researchers accomplish this is with the p-value. Whatever your personal feelings about the use of p-values, it’s important to understand and recognize specific instances when they can be misused. One way that I’ve seen this happen is while splitting a continuous ‘control’ variable into a categorical variable. While there can be justification for using this method, it’s generally considered a bad idea.\nBelow, I will use a simple example to both show the issue with and the attraction to using regression trees to dichotomize a continuous control variable. Before we get to the simulation, let’s imagine an example when a researcher might want to split a continuous variable.\n\n\nExample\nLet’s imagine a researcher is testing a therapy, recorded in the column cat as “A” or “B”. The researcher is trying to understand if this particular therapy has an effect on a clinical outcome, such as hospital length of stay, recorded in column val. However, there is a third variable that may influence how well the treatment affects patients (such as age) recorded in cont_cat. The researcher wants build a multi-variate linear regression to control for cont_cat. However, the cont_cat data is complex, maybe it’s easier to just split the data based off of the outcome variable. “This could take of non-linearity and call for a simpler interpretation,” the researcher thinks.\n\n\nSimulation\nWhile it sounds like an attractive proposition, it can get the researcher into trouble. The function below offers a simulation of what the researcher proposed:\nsim_function &lt;- function(samp_size = 1000, cut_ratio = .1, d = NULL) {\n  # If data is not provided, simulate it\n  if (is.null(d)) {\n    d &lt;-\n      data.frame(\n        cat = rep(c(\"A\", \"B\"), samp_size / 2),\n        cont_cat = rnorm(samp_size),\n        val = rlnorm(samp_size)\n      )\n  } else {\n    # If data is provided, sample it\n    d &lt;- d[sample(seq_len(nrow(d)), samp_size), ]\n  }\n\n  # Calculate the optimal cut point\n  r_cut &lt;-\n    tree(val ~ cont_cat,\n      data = d,\n      mindev = 0,\n      mincut = samp_size * cut_ratio\n    )$frame$splits[1, \"cutleft\"] %&gt;%\n    substring(2) %&gt;%\n    as.numeric()\n\n  # Create high/low variable based off the cut above\n  d$cut_var &lt;-\n    cut(\n      d$cont_cat,\n      breaks = c(min(d$cont_cat), r_cut, max(d$cont_cat)),\n      include.lowest = TRUE,\n      labels = c(\"low\", \"high\")\n    )\n\n  # Fit linear models\n  tree_mod &lt;- lm(val ~ cat + cut_var, data = d)\n  cont_mod &lt;- lm(val ~ cat + cont_cat, data = d)\n\n  # Return values signficant at alpha = .05\n  c(\n    \"Dichotomized\" = summary(tree_mod)$coefficients[\"cut_varhigh\", \"Pr(&gt;|t|)\"] &lt; .05,\n    \"Continuous\" = summary(cont_mod)$coefficients[\"cont_cat\", \"Pr(&gt;|t|)\"] &lt; .05,\n    \"Cut Value\" = r_cut\n  )\n}\nI won’t discuss the code above in detail, but the most important part is the call to the tree function (from the tree library). This function will find the optimal point to split the cont_var that best predicts val. The last rows output the results of testing for \\(\\alpha = 0.05\\) which means that our false positive rate should be controlled at 5%. The block of code below will check the significance form 1000 simulations to see if the false positive rates stays at that number.\nfull_sim &lt;- sapply(1:1000, function(x) {\n  sim_function(1000, cut_ratio = .1)\n})\n\nout &lt;- data.frame(`Precent Significant` = rowMeans(full_sim))\n\npander::pander(out, style = \"rmarkdown\")\n\n\n\n\n \nPrecent.Significant\n\n\n\n\nDichotomized\n0.445\n\n\nContinuous\n0.055\n\n\nCut Value\n-0.008577\n\n\n\n\nOh oh… Look at that rate of finding a significant result with the Dichotomized method! It’s 10x the prespecified \\(\\alpha\\) level! On the other hand, using the continuous value keeps us at the prespecified false positive rate.\nThis example clearly shows the issues that can come from an analysis that uses this technique. We know from the simulation that there is no relationship between the cont_val and the outcome variable val. However, in the extreme example above, we find a relationship almost 50% of the time.\n\n\nReal Data Simulation\nThe code below illustrates just why people do this. You can see that it is easier to find a relationship (in the case below it does actually exist).\ndata &lt;- read_tsv(\"AmesHousing.txt\")\n\nd &lt;- data %&gt;%\n  filter(\n    `House Style` %in% c(\"1Story\", \"2Story\"),\n    `Gr Liv Area` &lt; 4000\n  ) %&gt;%\n  select(\n    cat = `House Style`,\n    cont_cat = `Lot Area`,\n    val = SalePrice\n  )\n\nd$cat &lt;- ifelse(d$cat == \"1Story\", \"A\", \"B\")\n\nd$cont_cat &lt;- scale(d$cont_cat)\nd$val &lt;- scale(d$val)\n\n\nreal_sim &lt;- sapply(1:1000, function(x) {\n  sim_function(50, .1, d)\n})\n\nout &lt;- data.frame(`Precent Significant` = rowMeans(real_sim))\n\npander::pander(out, style = \"rmarkdown\")\n\n\n\n\n \nPrecent.Significant\n\n\n\n\nDichotomized\n0.971\n\n\nContinuous\n0.752\n\n\nCut Value\n0.1334\n\n\n\n\nAs you can see, we are able to detect an effect much more often when we dichotomize the variable. This shouldn’t come as too much of a surprise as we are effectively sacrificing a low positive rate for a high specificity. As always, there is no such thing as a free lunch.\nAs always, you can find all of the code for this post on github"
  },
  {
    "objectID": "posts/estimating-unique-values-in-a-set/index.html",
    "href": "posts/estimating-unique-values-in-a-set/index.html",
    "title": "Estimating the Unique Values in a Set",
    "section": "",
    "text": "Motivation\nA friend asked me to help him figure out some probabilities related to microwell plates that he was analyzing in his lab. A 90 well plate was filled from a mixed sample of an unknown number of unique particles. Let’s say there are a total of s total particles in the original sample. My friend said that he observed 88 unique particles in the 90 well plate. His question was simple: can we estimate the number of unique particles (s) in the original bag?\nWe can write this a little more succinctly:\n\nIf we draw 90 items from a set (with replacement) and we see 88 unique items, what is the probability that the set has s unique members? Can we calculate \\(P(s|x,t)\\) where x is the unique items observed out of a draw size t.\n\n\n\nSimulation\nLet’s get an idea of what the solution should look like by running a simulation. The example below will test various S values between 90 and 30,000 with 1,000 simulations per S size. For each size, we try to find the probability of drawing exactly 88 unique samples.\nsimXgS &lt;- function(s, t = 90, x = 88, num_samples = 1000) {\n  # Calculates the probability of drawing x unique samples\n  out &lt;- c()\n  for (i in 1:num_samples) {\n    samp &lt;- sample(1:s, size = t, replace = T) # draw 90\n    v &lt;- (length(unique(samp)) == x)\n    out &lt;- c(out, v)\n  }\n  mean(out)\n}\n\nset.seed(1)\nbag &lt;- seq(from = 90, to = 20000, by = 50) # S Values\n\nfull_out &lt;- sapply(bag,\n                   simXgS,\n                   t = 90,\n                   x = 88,\n                   num_samples = 1000)\n\np_full_out &lt;- full_out / sum(full_out)\n\ndf_sim &lt;-\n  data.frame(S = bag, pdf = p_full_out)\nbest_S &lt;- df_sim$S[which.max(df_sim$pdf)]\nSome of you may have made the correct observation that we are actually calculating \\(P(x|s,t)\\). That is what full_out represents. However, as you will see below, we can use Bayes Rule to show that this is valid when we scale by the sum of all values (p_full_out). In other words: \\[ P(s|x,t) = \\frac{P(x|s,t)}{\\sum_s{P(x|s,t)}}\\]\nThe graph below shows the scaled probability of the true number of unique values in the original sample. The labeled vertical line shows the best guess for the value of s based off the simulations.\n\nWe can even run the simulation multiple times (code not shown) to get a better estimate of s. After 100 simulations, the best s is 2006 (95% CI [1968, 2045]). The plot below shows the distribution of these values.\n\n\n\nAnalytic Solution\nNow let’s see if we can calculate \\(P(s|x,t)\\) without using simulation. We can start with Bayes Rule: \\[ P(s|x,t) = \\frac{P(x|s,t)P(s|t)}{P(x|t)}\\] We can calculate the formula for \\(P(x|s,t)\\) by using combinatorics: \\[ P(x|s,t) \\propto \\frac{s!}{s^{x}(s-x)!}*\\frac{\\binom{t-1}{t-x}}{s^{t-x}}\\]\nNote the ‘proportional’ sign that’s used instead of an ‘equal’ sign. I will show why this is not important shortly. Let’s look at \\(P(s|t)\\). First we notice that s is entirely independent of t. (I can use a plate of any size for testing, it probably relates to the capabilities of the device measuring the wells, not s). Additionally, we can give s a uniform distribution; It’s equally likely to be any number in the range we are testing.\n\\[ P(s|t) = P(s) = \\frac{1}{q}\\]\nWhere q equals the whole range we are testing s over (length(bag)).\nLastly we turn to \\(P(x|t)\\), which is the marginal distribution of x over all of the possible values of s.\n\\[\\begin{aligned}P(x|t) &=  \\int_s{P(x|s,t)P(s|t)ds} \\\\\\\\\n& = \\int_s{P(x|s,t)\\frac{1}{q}ds} \\\\\\\\\n& = \\frac{1}{q}\\int_s{P(x|s,t)ds} \\\\\\\\\\\n& = \\frac{1}{q}\\sum_s{P(x|s,t)}\n\\end{aligned}\n\\] Now let’s put it all together: \\[\\begin{aligned}\nP(s|x,t) &= \\frac{P(x|s,t)P(s|t)}{P(x|t)} \\\\\\\\\n\\\\\n&= \\frac{P(x|s,t)\\frac{1}{q}}{\\frac{1}{q}\\sum_s{P(x|s,t)}}\\\\\\\\\n\\\\\n& = \\frac{P(x|s,t)}{\\sum_s{P(x|s,t)}}\n\\end{aligned}\n\\] This result accomplished two goals:\n\nIt justifies dividing by the sum of all the probabilities (as we did in the first section).\nIt means that our analytic solution to \\(P(x|s,t)\\) can be proportional to the true value (as long as it does not contain an s). Any proportions like that will act as the \\(\\frac{1}{q}\\) did and get cancelled out.\n\nBelow I’ve written a function for calculating \\(P(x|s,t)\\) where t = plates and x = observed_unique.\nprobXgS &lt;- function(s,\n                    t = 90,\n                    x = 88) {\n  m_vs &lt;- t - x\n  prod(s:(s - (x - 1)) / s) *\n    choose(x + m_vs - 1, m_vs) *\n    1 / s ^ m_vs\n}\n\ncomp_out &lt;- sapply(bag, probXgS, x = 88, t = 90)\np_comp_out &lt;- comp_out / sum(comp_out)\n\ndf_sim$calc_pdf &lt;- p_comp_out\nbest_S_calc &lt;-\n  df_sim$S[which.max(df_sim$calc_pdf)]\nThe graphs show that the analytic solution (red) correctly approximates the probability distribution of the simulation. The most likely value for \\(s\\) is also shown.\n\nLastly, we can look at the performance to the simulation more directly by plotting the residuals:\n\nThe residuals are fairly constant for all calculated values. There is naturally bunching near 0 because of chosen values for s.\n\n\nConclusion\nThis post shows my method for solving these sort of problems. Doing simulations like I showed in the first part really makes the problem, and statistics, come alive. Once I have a good understanding of the actual problem, I like to think of an analytic solution that can be generalized to other forms of the same problem. Below I show some example where the number of plates and the unique observations differs from those of the original problem. You can also fine all the code for this blog post on my GitHub."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "West End Statistics is a statistical consulting company focused on designing and analyzing clinical trial data. Learn more about the our background or see our posts to learn about our research interests."
  },
  {
    "objectID": "posts/conditional-mle-variance/index.html",
    "href": "posts/conditional-mle-variance/index.html",
    "title": "Conditional MLE Variance",
    "section": "",
    "text": "Introduction\nThe most challenging (and useful) work in statistics comes not from trying to estimate a value, but from estimating the accuracy of that value. The most famous example of this is that for a normally distributed variable \\[\\hat{\\mu} \\sim  \\mathcal{N}\\left( \\mu, \\sigma^2/n\\right)\\] where \\(\\hat{\\mu}\\) is an estimate of \\(\\mu\\) and \\(n\\) is the number of samples taken. The formula above is actually a generalization of \\[\\hat{\\theta} \\sim  \\mathcal{N}\\left( \\theta, 1/(n\\mathcal{I}_\\hat{\\theta})\\right)\\] where \\(\\mathcal{I}_\\hat{\\theta}\\) is the Fisher Information for the Normal Distribution. \\(\\hat{\\theta}\\) can represent more than one parameter to be estimated. In our specific example, \\(\\mathcal{I}_\\hat{\\theta} = 1/\\sigma^2\\). One important note about \\(\\mathcal{I}_\\hat{\\theta}\\) is that it does not explicitly depend on the data being analyzed. As a result of this issue, Fisher proposed a slightly different statistic, called the Observed Fisher Information (\\(I(x)\\)) which depends on the data observed. \\[ I\\left(x\\right) = -\\ddot{l}_x(\\hat{\\theta}) = -\\frac{\\partial^2}{\\partial \\theta^2}l_x\\left(\\theta\\right)\\vert_{\\hat\\theta}\\] where \\(l_x\\left(\\theta \\right)\\) is the log-likelihood function for the distribution that the data is drawn from and is evaluated at the maximum likelihood estimator \\(\\hat\\theta\\). \\(I(x)\\) and \\(\\mathcal{I}_\\hat{\\theta}\\) are related by the fact the \\[E[I(x)] = n\\mathcal{I}\\] The section above was adapted from Chapter 4.3 in the excellent Computer Age Statistical Inference which explains maximum likelihood (ML) in the most accessible way I’ve seen. What follows next is a recreation of Figure 4.2 (pg 48) which illustrates the difference in the Conditional (Observed) Fisher Information and the Expected Fisher Information\n\n\nSimulation Comparing Observed and Expected Fisher Information\n\nEstimating \\(\\hat{\\theta}\\) using MLE\nThe example that follows is for the Cauchy distribution which follows the distribution \\[f_\\theta(x) = \\frac{1}{\\pi} \\frac{1}{1+(x-\\theta)^2}\\] This is a form of the Cauchy distribution where the scale parameter equals 1. The likelihood equation is \\[-log(f_\\theta(x)) = -(log(\\frac{1}{\\pi}) + log(1) -log(1 + (x - \\theta)^2))\\] We can then drop all of the constant terms and our final equation for calculating the maximum likelihood is \\[ l_x(\\theta) = \\sum_{i=1}^{n}{log(1 + (x_i - \\theta)^2)}\\] which is defined in R here:\n\nlcauchy &lt;- function(theta, x){\n  sum(log(1 + (x - theta)^2))\n}\n\nWe can then use the nlm function to find the MLE for \\(\\theta\\). location in the code below is our true \\(\\theta\\).\n\nset.seed(1120)\ncauchy_samp &lt;- rcauchy(n = 20, location = 0, scale = 1)\nmle_estimate &lt;- nlm(lcauchy, p = median(cauchy_samp), \n                    x = cauchy_samp)\n\nround(mle_estimate$estimate,2)\n\n[1] 0.13\n\n\n\n\nCalculating Expected Fisher Information\nThe Expected Fisher Information can be shown to be \\[\\mathcal{I}_\\hat{\\theta} = \\frac{1}{2}\\] meaning the the variance of the estimate of \\(\\hat{\\theta}\\) is bound by variance of \\[\\frac{1}{n\\mathcal{I}} = \\frac{2}{n}\\]. The full proof can be found in solution (i) here. Note that this does not depend on the data observed (only the number of point \\(n\\))\n\n\nCalculating the Observed Fisher Information\nThe observed Fisher Information may be calculated based off its definitions above. For the Cauchy distribution \\[ I(x) = -2\\sum_{i=1}^{n}{\\frac{(x_i-\\hat\\theta)^2-1}{1+(x_i-\\hat\\theta)^2)^2}}\\] which is defined below in R:\n\nobsInformCauchy &lt;- function(theta, x){\n  sum((-2 * ((x - theta) ^ 2 - 1)) / (1 + (x - theta) ^ 2) ^ 2)\n}\n\nThis suggests that \\(\\hat{\\theta}\\) is bound by variance of \\[\\frac{1}{I(x)}\\] As before please note that this does depend on the data collected.\n\n\nSimulation comparing the three variances\nThe simulation below will create 10,000 samples of 20 from a Cauchy distribution.\n\nvarCauchy &lt;- function(n) {\n  # Create a sample size n from cauchy distribution\n  cauchy_samp &lt;- rcauchy(n)\n  \n  # Compute the MLE of theta\n  mle_estimate &lt;-\n    nlm(f = lcauchy, \n        p = median(cauchy_samp),\n        x = cauchy_samp)\n  theta_hat &lt;-  mle_estimate$estimate\n  \n  # Calculate the Observed Fisher Information at the MLE of theta\n  fish &lt;- obsInformCauchy(theta_hat, cauchy_samp)\n  \n  # Return the MLE and Observed Fisher Information\n  c(t_h = theta_hat,\n    obs_fish = fish)\n}\n\nlibrary(dplyr)\nsims &lt;- sapply(1:10000, function(x) varCauchy(20)) %&gt;% \n  t %&gt;% \n  as.data.frame\n\npander::pander(head(sims), style = 'rmarkdown')\n\n\n\n\nt_h\nobs_fish\n\n\n\n\n-0.07252\n8.568\n\n\n0.07057\n12.93\n\n\n-0.149\n6.096\n\n\n-0.08777\n9.966\n\n\n0.1799\n7.437\n\n\n0.5064\n11.2\n\n\n\n\n\nWe will then calculate the variance based off the observed fisher (column bound below). Finally, we will group the the variance into deciles group and calculate the actual variance from the MLE (var_mle).\n\ndf &lt;- sims %&gt;% \n  mutate(bound = 1/obs_fish, \n         group = cut(bound, \n                     breaks = quantile(bound, \n                                       probs = seq(0,1,.1)),\n                     ordered_result = T)) %&gt;% \n  group_by(group) %&gt;% \n  summarise(avg_bound = mean(bound),\n            var_mle = var(t_h))\n\npander::pander(df, style = 'rmarkdown')\n\n\n\n\ngroup\navg_bound\nvar_mle\n\n\n\n\n(0.0407,0.0652]\n0.059\n0.07185\n\n\n(0.0652,0.0728]\n0.06929\n0.07661\n\n\n(0.0728,0.0798]\n0.0764\n0.0852\n\n\n(0.0798,0.0868]\n0.0832\n0.09435\n\n\n(0.0868,0.094]\n0.09029\n0.1075\n\n\n(0.094,0.102]\n0.09799\n0.1125\n\n\n(0.102,0.114]\n0.1077\n0.1199\n\n\n(0.114,0.129]\n0.1213\n0.1323\n\n\n(0.129,0.157]\n0.1411\n0.1537\n\n\n(0.157,1.15]\n0.2163\n0.244\n\n\nNA\n0.0407\nNA\n\n\n\n\n\nWe will then plot the average estimated variance avg_bound against the actual variance var_mle along with the variance given by the expected fisher information.\n\nlibrary(ggplot2)\np &lt;- df %&gt;% \n  ggplot(aes(x = avg_bound, y = var_mle, color = \"Simulated Values\")) + \n  geom_point() + \n  geom_hline(aes(yintercept = 0.1, \n             color = \"Expected Value\"),\n             linetype = 2,\n             show.legend = T)\n\n\n\n\n\n\nThe graph above clearly shows that the Observed Information Bound is much better at assessing MLE variability. It’s important to note that as \\(n\\) gets larger, this difference becomes less and less important. Nevertheless, this is a great argument for taking the actual data into account when computing things like confidence intervals.\n\n\n\nFurther Reading\n\nComputer Age Statistical Inference provided the initial reason for exploring this on page 48. Additionally, an older manuscript (1978!) from the same authors is available for free online. See the graph on page 460 of the pdf.\n\nGeyer, 2003 provided some much needed details on how to calculate the fisher information in R.\nA step by step proof on how to calculate the Expected Fisher Information (warning: it’s awful) can be found here in the solution to (i)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Cutting with Trees\n\n\n\n\n\n\n\nCART\n\n\nR\n\n\nsimulation\n\n\np value\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2017\n\n\n\n\n\n\n  \n\n\n\n\nEstimating the Unique Values in a Set\n\n\n\n\n\n\n\nR\n\n\ncombinatorics\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2016\n\n\n\n\n\n\n  \n\n\n\n\nConditional MLE Variance\n\n\n\n\n\n\n\nR\n\n\nsimulation\n\n\nMLE\n\n\ncasi\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2016\n\n\n\n\n\n\nNo matching items"
  }
]